{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 필수 패키지 설치 및 환경변수 로드\n",
    "\n",
    "- 패키지 설치 전 가상환경 활성화 필요\n",
    "  - Mac : source rag-env/bin/activate\n",
    "  - Window : rag-env\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U langchain-core langchain-community langchain-openai huggingface_hub python-dotenv ipdb langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. OpenAI GPT-3.5-turbo 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OpenAI GPT-3.5-turbo] 테스트 중...\n",
      "답변: content='RAG(Retrieval-Augmented Generation)는 검색과 생성을 결합한 모델로, 검색된 정보를 활용하여 생성 작업을 수행하는 구조를 가지고 있다. 이 모델은 두 가지 주요 구성 요소로 이루어져 있다.\\n\\n1. Retrieval 모듈: 검색된 정보를 가져오는 역할을 하는 모듈이다. 이 모듈은 미리 구축된 대규모의 지식 베이스나 검색 엔진을 활용하여 특정 주제에 대한 정보를 검색하고 가져온다.\\n\\n2. Generation 모듈: 검색된 정보를 활용하여 원하는 결과물을 생성하는 역할을 하는 모듈이다. 이 모듈은 검색된 정보를 바탕으로 자연어 생성 모델을 활용하여 새로운 문장이나 텍스트를 생성한다.\\n\\nRAG는 이 두 모듈을 효과적으로 결합하여 검색된 정보를 활용하여 자연어 생성 작업을 수행할 수 있도록 한다. 이를 통해 보다 정확하고 의미 있는 결과물을 생성할 수 있게 되며, 다양한 응용 분야에서 활용될 수 있는 모델이다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 343, 'prompt_tokens': 27, 'total_tokens': 370, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BTkKLfAzvZUfoM9j52c6i2mJY7ubr', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--60334cb6-e015-44e1-905d-7f6df3499f61-0' usage_metadata={'input_tokens': 27, 'output_tokens': 343, 'total_tokens': 370, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(\"[OpenAI GPT-3.5-turbo] 테스트 중...\")\n",
    "openai_model = ChatOpenAI(temperature=0.3)\n",
    "print(\"답변:\", openai_model.invoke(\"RAG(Retrieval-Augmented Generation)의 구조를 설명해줘.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 로컬 모델 테스트: Ollama \n",
    "### Ollama란?\n",
    "- Ollama는 로컬에서 LLM을 실행할 수 있게 해주는 툴입니다.  \n",
    "- 인터넷 없이도 PC에 모델을 다운로드해 사용 가능하며,  FastChat이나 HuggingFace Transformers보다 훨씬 간단하게 로컬 LLM을 실행할 수 있도록 도와줍니다.\n",
    "\n",
    "### 사전 준비 (최초 1회만 수행)\n",
    "\n",
    "1. 공식 웹사이트에서 Ollama 설치  \n",
    "   - https://ollama.com/download (macOS, Windows, Linux 지원)\n",
    "\n",
    "2. 터미널에서 모델 다운로드 (예: Mistral)\n",
    "   ```bash \n",
    "   ollama pull mistral\n",
    "- 예: mistral 모델 테스트시 Ollama가 설치되어 있고 'ollama run mistral' 실행 상태여야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ollama mistral] 로컬 테스트 중...\n",
      "답변: content='Retrieval-Augmented Generation (RAG)은 인공지능 모델에서 사용되는 새로운 작업 방법으로, 기존의 Transformer 모델(예: BERT, T5)보다 더 효율적이고 정확한 대화 생성을 위해 구현되었습니다. RAG는 두 가지 주요 구성 요소로 이루어져 있습니다: Retriever 및 Generator.\\n\\n1. Retriever: Retriever는 대화 상황에 적합한 데이터를 검색하는 역할을 합니다. 주로 전체 코뱃의 정보 또는 문서 중에서 필요한 정보를 검색합니다. Retriever는 가장 적절한 정보를 선택하여 이를 Generator에게 전달합니다.\\n\\n2. Generator: Generator는 전달받은 정보와 사용자의 질문과 함께 대화 응답을 생성하는 역할을 합니다. Generator는 기존 Transformer 모델처럼 encoder-decoder 구조를 가지며, Retriever로부터 전달받은 정보와 함께 학습되어 더 정확하고 효율적인 대화 생성이 가능합니다.\\n\\nRAG의 장점으로는 대화 상황에서 기존 Transformer 모델보다 정확하고 효율적인 대화 생성을 수행할 수 있다는 것입니다. 이를 위해 Retriever가 필요한 정보를 선택해서 제공함으로써 Generator가 부족한 정보 또는 오작동을 방지하고, 전체 코뱃에서 필요한 정보를 찾아내어 응답하도록 할 수 있습니다.' additional_kwargs={} response_metadata={'model': 'mistral', 'created_at': '2025-05-05T07:17:18.827897Z', 'done': True, 'done_reason': 'stop', 'total_duration': 33176949666, 'load_duration': 7493916, 'prompt_eval_count': 30, 'prompt_eval_duration': 1984190792, 'eval_count': 563, 'eval_duration': 31184206583, 'model_name': 'mistral'} id='run--f43ca49c-f3d0-4d20-a656-37b1709a3c90-0' usage_metadata={'input_tokens': 30, 'output_tokens': 563, 'total_tokens': 593}\n"
     ]
    }
   ],
   "source": [
    "print(\"[Ollama mistral] 로컬 테스트 중...\")\n",
    "try:\n",
    "    ollama_model = ChatOllama(model=\"mistral\")\n",
    "    print(\"답변:\", ollama_model.invoke(\"RAG(Retrieval-Augmented Generation)의 구조를 설명해줘.\"))\n",
    "except Exception as e:\n",
    "    print(\"Ollama 실행 오류:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"모든 모델 테스트 완료! 어떤 모델이 가장 적절했는지 비교해보세요.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
